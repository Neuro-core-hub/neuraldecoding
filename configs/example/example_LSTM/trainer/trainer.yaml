defaults:
  - /model@model: model

optimizer:
  type: AdamW
  params:
    lr: 0.0002
    weight_decay: 0.001

scheduler:
  type: LinearLR
  params:
    start_factor: 1.0
    end_factor: 0.05 #1e-5 / 2e-4
    total_iters: 20

loss_func:
  type: MSELoss
  params: {}

training:
  num_epochs: 20
  batch_size: 64
  device: cuda # cuda , if avail 
  print_results: True
  print_every: 5
  clear_cache: False

evaluation:
  metrics: ["loss", "correlation"]
  params: {}

data: {}